---
title: "Application to Gene Expression Data"
output: html_document
---

Load the library 

```{r}
library(ISLR)
library(e1071)
```

Examine the `Khan` data set, which consists of a number of tissue samples corresponding to four distinct types of small round blue cell tumors.

There is a gene expression measurements for each tissue sample.

The data consists of training data and testing data.

```{r}
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
```

This data set consists of expression measurements for 2,308 genes. The training and test sets consist of 63 and 20 observations respectively.

There are 63 observations and each observation has 2,380 genes.

```{r}
Khan$xtrain[63, 2308]
```

That is the training dataset, observation 63, the 2308th gene and it won't go over to 2309th.

```{r}
table(Khan$ytrain)
table(Khan$ytest)
```

We will use a support vector approach to predict cancer subtype using gene expression measurements. (It means the data frame has 63 rows and 2380 columns)

In this data set, there are a very large number of features relative to the number of observations.

This suggests that we should use a linear kernel, because the additional flexibility that will result from using a polynomial or radial kernel is unnecessary.


```{r}
dat <- data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out <- svm(y~., data=dat, kernel="linear",cost=10)
summary(out)
table(out$fitted, dat$y)
```

We see that there are no training errors. In fact, this is not surprising, because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes.

We are most interested not in the support vector classifierâ€™s performance on the training observations, but rather its performance on the test observations.

```{r}
dat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te <- predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
```

We see that using `cost=10` yields two test set errors on this data.

Should we use a different cost?