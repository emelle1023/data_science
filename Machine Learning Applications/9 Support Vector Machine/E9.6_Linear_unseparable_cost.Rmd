---
title: "Small VS large cost of SVM for barely lineraly separable data"
output: html_document
---

## Synopsis

We will investiagate the claim of small value of `cost` VS large value of `cost` of Support Vector Machine for data that is just barely linearly separable.

## Analysis

### Load library

```{r}
library(e1071)
```

### Genearte two variables of 2 classes that are barely linearly separable

We randomly generate 1000 points and scatter them across lin $x=y$ with wide margin. We also 
create noisy points along the line $5x-4y-50=0$.

These points make the classes barely separable and also shift the maximum margin classifier.

```{r}
set.seed(3154)

# There are two classifications 1 and 0

# Class one (Classification 1)
x.one = runif(500, 0, 90)  # create 500 samples between 0 and 90
y.one = runif(500, x.one + 10, 100) # just make the other variable 10 more
x.one.noise = runif(50, 20, 80) # introduce 50 errors between 20 and 80
y.one.noise = 5/4 * (x.one.noise - 10) + 0.1 # the other noise variable is in a linear function

# Class zero (Classification 0)
x.zero = runif(500, 10, 100) # the other clssification is 10 more
y.zero = runif(500, 0, x.zero - 10) # cutting them down
x.zero.noise = runif(50, 20, 80)
y.zero.noise = 5/4 * (x.zero.noise - 10) - 0.1

# Combine all
class.one = seq(1, 550)
x = c(x.one, x.one.noise, x.zero, x.zero.noise)
y = c(y.one, y.one.noise, y.zero, y.zero.noise)
```

This is how we make barely linear separable data. Both using noise and a straight line function.

#### Plot of the two classifications of barely linear seperable data.

```{r}
plot(x[class.one], y[class.one], col = "blue", pch = "+", ylim = c(0, 100))
points(x[-class.one], y[-class.one], col = "red", pch = 4)
```

It is linear and both classifications are very close together. A clever way to do this. The plot shows that classes are barely separable. The noisy points create a fictitious boundary $5x-4y-50=0$.



### Compute cross-validation error rates for SVM with a range of `cost` values

We create a z variable according to classes.

```{r zvariable, cache=TRUE}
set.seed(555)
z = rep(0, 1100)
z[class.one] = 1
data = data.frame(x = x, y = y, z = z)
tune.out = tune(svm, as.factor(z) ~ ., 
                data = data, kernel = "linear", 
                ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)))
summary(tune.out)
```

```{r}
data.frame(cost = tune.out$performances$cost, 
           misclass = tune.out$performances$error * 1100) # 1100 are the total samples together
```

The table above shows train-misclassification error for all costs. A cost of 10000 seems to classify all points correctly. This also corresponds to a cross-validation error of 0.

### Generate an appropariate test data set and compute the test errors corresponding to each of the values of `cost` cosidered.

#### Generate the data 

We now generate a random test-set of same size. This test-set satisfies the true decision boundary $x=y$

```{r}
set.seed(1111)

# create 1000 samples 
x.test = runif(1000, 0, 100) # 1000 samples between 0 and 100
class.one = sample(1000, 500) # 500 samples < 1000 values
y.test = rep(NA, 1000) # 1000 of NA

# Fill up all y.test

# Set y > x for class.one 
# put x.test into the first 500 y.test
for (i in class.one) {
    y.test[i] = runif(1, x.test[i], 100) # put a value > x.test value current pos
}

# set y < x for class.zero
# Put the rest of x.test into the remaining 500 y.test
for (i in setdiff(1:1000, class.one)) {
    y.test[i] = runif(1, 0, x.test[i]) # put a value < x.test value current pos
}
```

This is a funny way to populate y.test, but after all, it is stil random.

#### Plot of the two classifications

```{r}
plot(x.test[class.one], y.test[class.one], col = "blue", pch = "+")
points(x.test[-class.one], y.test[-class.one], col = "red", pch = 4)
```

The testing points looks seperable though.

#### Make the predictions

We now make same predictions using all linear svms with all costs used in previous part.

```{r}
set.seed(30012)
z.test = rep(0, 1000)
z.test[class.one] = 1
all.costs = c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)
test.errors = rep(NA, 8)

data.test = data.frame(x = x.test, y = y.test, z = z.test)
for (i in 1:length(all.costs)) {
    svm.fit = svm(as.factor(z) ~ ., data = data, kernel = "linear", cost = all.costs[i])
    svm.predict = predict(svm.fit, data.test)
    test.errors[i] = sum(svm.predict != data.test$z)
}

data.frame(cost = all.costs, `test misclass` = test.errors)
```

`cost=10` seems to be performing better on test data, making the least number of classification errors. This is much smaller than optimal value of 10000 for training data.



## Conclusion

We again see an overfitting phenomenon for linear kernel. A large cost tries to fit correctly classify noisy-points and hence overfits the train data. A small cost, however, makes a few errors on the noisy test points and performs better on test data.

We should only be relying on testing data. It looks like `cost-10` works better, we don't need a huge `cost=10000` of higher for testing data.
