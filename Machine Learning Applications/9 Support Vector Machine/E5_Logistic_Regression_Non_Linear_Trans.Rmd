---
title: "Logistic Regression non-linear transformation of SVM Kernel on Non-Linear Data"
output: html_document
---

## Synopsis

To preform classification using a non-linear decision boundary, a SVM non-linear kernel can be used, but we can also use logistic regression and use the non-linear transformation feature to perform classification.

## Analysis

### Data set generation and loading

Generate a data set with *`n = 500`* and *`p = 2`* with two classes of a quadratic decision boundary.

```{r}
library(e1071) # load the SVM packages

set.seed(421)
x1 <- runif(500) - 0.5 # Just 500 random number between 0 and 1
x2 <- runif(500) - 0.5
y <- 1 * (x1^2 - x2^2 > 0) # returns TRUE or FALSE and gets 1 & 0
```

But we don't have any specific function, jsut two vectors of random numbers.

### Plot observations of the original dataset

Colored according to their class labels.

```{r}
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
```

The plot clearly shows non-linear decision boundary, given by the code.

### Linear Logistics Regression model

```{r}
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
```

Based on the p-values, both variables are insignificant for predicting y. Bear in mind that this may not work. But we continue to predict using the test dataset.

#### Linear Logistics Regression prediction on training data

We will need to put the data in a data frame first.

```{r}
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.52, 1, 0) # Use threshold 0.52 instead of 0.5
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]

# Plot only on the training dataset
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```

The training dataset (from the dataframe) is the same as the observation. In fact, we use just every observation we have.

With the given model and a probability threshold of 0.5, all points are classified to single class and no decision boundary can be shown. Hence we shift the probability threshold to 0.52 to show a meaningful decision boundary. This boundary is linear as seen in the figure.

Since our model is linear, the boundary is linear, but our data is not linear, this may not be accurate or correct.

### Non-linear Logistics Regression model

We will use and play with a non-linear logistics regression.

In specific, we use squares, product interaction terms to fit the model.

```{r}
lm.fit = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2), 
             data = data, family = binomial)
```

Unfortunately, we don't know under what rule we will pick interaction of degrees.

#### Non-Linear Logistics regression prediction

We will also plot the prediction, and this is non-linear 

```{r}
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```


This non-linear decision boundary closely resembles the true decision boundary. In another word, we simply cannot classify with Linear Logistics approach.

Remember, we also plot the plot the true boundary (which shows significant classification), and our linear logistics cannot even classify.

### Support Vector Machine Non-Linear kernel `gamma=1`

A non-linear model may work.

```{r}
svm.fit = svm(as.factor(y) ~ x1 + x2, data, gamma = 1)
svm.pred = predict(svm.fit, data)
data.pos = data[svm.pred == 1, ]
data.neg = data[svm.pred == 0, ]
```

#### Non-Linear Support Vector Machine kernel

```{r}
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```


We haven't loaded SVM polynomail this time, but the kernel method is promising and it can classify the data.

Again, the non-linear decision boundary on predicted labels closely resembles the true decision boundary.

Remember, our data represents a non-linear pattern, and our non-linear kernel predicts the same thing.

## Conclusion

This experiment enforces the idea that SVMs with non-linear kernel are extremely powerful in finding non-linear boundary. Both, logistic regression with non-interactions and SVMs with linear kernels fail to find the decision boundary. Adding interaction terms to logistic regression seems to give them same power as radial-basis kernels. However, there is some manual efforts and tuning involved in picking right interaction terms. This effort can become prohibitive with large number of features. Radial basis kernels, on the other hand, only require tuning of one parameter - gamma - which can be easily done using cross-validation.


