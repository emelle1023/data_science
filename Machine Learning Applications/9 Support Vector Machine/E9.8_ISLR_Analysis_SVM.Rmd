---
title: "ISLR Analysis using Support Vector Machine"
output: html_document
---

## Synopsis

Analyse `OJ` data from iSRL packages using various Support Vector Machine approaches.

## Analysis

### (a) Create random training and testing datasets

#### Load ISLR library 

```{r}
library(ISLR)
library(e1071)
```

```{r}
set.seed(9004)
train = sample(dim(OJ)[1], 800) # total 800 samples from 0-1070 (random)
OJ.train = OJ[train, ] # Get random 800 OJ for training (selected randomly)
OJ.test = OJ[-train, ] # Get the rest of 270 for testing
```

Random selection of 800 from 1070 observations, but is this a cross-validation method?

### (b) Fit using Support Vector Classifier (linear) 

```{r}
svm.linear = svm(Purchase ~ ., kernel = "linear", data = OJ.train, cost = 0.01) # stick with 0.01
summary(svm.linear)
```

Support vector classifier creates 432 support vectors out of 800 training points. Out of these, 217 belong to level ùô≤ùô∑ and remaining 215 belong to level ùôºùôº.


### (c) Check the training and Test error

#### (c.1) Training Error

```{r}
train.pred = predict(svm.linear, OJ.train)
table(OJ.train$Purchase, train.pred)
```

To get the training error, we do $\frac{82+53}{439 + 53 + 82 + 226}$

```{r}
(82 + 53)/(439 + 53 + 82 + 226)
```

The Training error is $16.875\%$

#### (c.2) Testing Error

```{r}
test.pred = predict(svm.linear, OJ.test)
table(OJ.test$Purchase, test.pred)
```

To get the testing error, we do $\frac{19+29}{142+19+29+80}$

```{r}
(19 + 29)/(142 + 19 + 29 + 80)
```

The Testing error is $17.78\%$

The Testing error is only slightly larger than training error.

### (d) Select optimal model based on optimal cost from 0.01 to 10

Select a range of costs

```{r lin, cache=TRUE}
set.seed(1554)
tune.out = tune(svm, Purchase ~ ., data = OJ.train, kernel = "linear", 
                ranges = list(cost = 10^seq(-2, 1, by = 0.25))) # alternatively way 
summary(tune.out)
```

Tuning shows that optimal cost is 0.3162

### (e) Compute training and testing error rates of Linear model using new value of cost

#### (e.1) Training Error rate with new cost

```{r}
svm.linear = svm(Purchase ~ ., kernel = "linear", data = OJ.train, 
                 cost = tune.out$best.parameters$cost) # new cost
train.pred = predict(svm.linear, OJ.train)
table(OJ.train$Purchase, train.pred)
```

```{r}
(57 + 71)/(435 + 57 + 71 + 237)
```

#### (e.2) Testing error rate with new cost

```{r}
test.pred = predict(svm.linear, OJ.test)
table(OJ.test$Purchase, test.pred)
```

```{r}
(29 + 20)/(141 + 20 + 29 + 80)
```

The training error decreases to 16% but test error slightly increases to $18.1\%$ by using best cost. It is not necessary better to use the new optmial cost.

### (f) Fit using Support Vector Machine with a radial kernel of default `gamma`

We don't specifiy any gamma value for default.

```{r}
set.seed(410)
svm.radial = svm(Purchase ~ ., data = OJ.train, kernel = "radial")
summary(svm.radial)
```

#### (f.1) Training error with default gamma

```{r}
train.pred = predict(svm.radial, OJ.train)
table(OJ.train$Purchase, train.pred)
```

```{r}
(40 + 78)/(452 + 40 + 78 + 230)
```

#### (f.2) Testing error with default gamma

```{r}
test.pred = predict(svm.radial, OJ.test)
table(OJ.test$Purchase, test.pred)
```

```{r}
(27 + 15)/(146 + 15 + 27 + 82)
```

The radial basis kernel with default gamma creates 367 support vectors, out of which, 184 belong to level ùô≤ùô∑ and remaining 183 belong to level ùôºùôº. The classifier has a training error of $14.7\%$ and a test error of $15.6\%$ which is a slight improvement over linear kernel. We now use cross validation to find optimal gamma.

Maybe a non-linear model is the appropriate choice of model regardless of any tuning.

#### (f.3) Apply a range of costs for the radial model with default gamma

Radial seems like a good model, now find the optmial cost for radial model.

```{r rad, cache=TRUE}
set.seed(755)
tune.out = tune(svm, Purchase ~ ., data = OJ.train, kernel = "radial", 
                ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)
```

cost = 0.31623 seems to produce the least cross-validation error.


#### (f.4) Check for training error for radial model best cost 

```{r}
svm.radial = svm(Purchase ~ ., data = OJ.train, kernel = "radial", 
                 cost = tune.out$best.parameters$cost)
train.pred = predict(svm.radial, OJ.train)
table(OJ.train$Purchase, train.pred)
```

```{r}
(77 + 40)/(452 + 40 + 77 + 231)
```
#### (f.5) Check for testing error for radial model best cost 

```{r}
test.pred = predict(svm.radial, OJ.test)
table(OJ.test$Purchase, test.pred)
```

```{r}
(28 + 15)/(146 + 15 + 28 + 81)
```

Tuning slightly decreases training error to $14.6\%$ and slightly increases test error to $16\%$ which is still better than linear kernel.

So, it looks like we don't really need to choose the best costs.


### (g) Fit using Support Vector Machine with a polynomial of `degree=2`

```{r}
set.seed(8112)
svm.poly = svm(Purchase ~ ., data = OJ.train, kernel = "poly", degree = 2)
summary(svm.poly)
```

#### (g.1) Check the training error

```{r}
train.pred = predict(svm.poly, OJ.train)
table(OJ.train$Purchase, train.pred)
```

```{r}
(32 + 105)/(460 + 32 + 105 + 203)
```


#### (g.2) Check the testing error

```{r}
test.pred = predict(svm.poly, OJ.test)
table(OJ.test$Purchase, test.pred)
```

```{r}
(12 + 37)/(149 + 12 + 37 + 72)
```

Summary shows that polynomial kernel produces 452 support vectors, out of which, 232 belong to level ùô≤ùô∑ and remaining 220 belong to level ùôºùôº. This kernel produces a train error$ of 17.1\%$ and a test error of $18.1\%$ which are slightly higher than the errors produces by radial kernel but lower than the errors produced by linear kernel.

Polynomial never seems to outpreform radial.


#### (g.3) Select the optimal polynomial model based on optimal cost for degree=2

```{r pol, cache=TRUE}
set.seed(322)
tune.out = tune(svm, Purchase ~ ., data = OJ.train, kernel = "poly", degree = 2, 
    ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)
```

#### (g.4) Check Training error for polynomial degree=2 optimal cost

```{r}
svm.poly = svm(Purchase ~ ., data = OJ.train, kernel = "poly", 
               degree = 2, cost = tune.out$best.parameters$cost)
train.pred = predict(svm.poly, OJ.train)
table(OJ.train$Purchase, train.pred)
```

```{r}
(37 + 84)/(455 + 37 + 84 + 224)
```


#### (g.5) Check Testing error for polynomial degree=2 optimal cost

```{r}
test.pred = predict(svm.poly, OJ.test)
table(OJ.test$Purchase, test.pred)
```

```{r}
(13 + 34)/(148 + 13 + 34 + 75)
```


Tuning reduces the training error to $15.12\%$ and test error to $17.4\%$ which is worse than radial kernel but slightly better than linear kernel.

Even after tuning, polynomial still not very good.


## (h) Conclusion


Overall, radial basis kernel seems to be producing minimum misclassification error on both train and test data.


