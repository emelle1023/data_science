---
title: "Model selection using validation set approach and cross-validation"
output: html_document
---

## Synopsis

To choose among Models Using the Validation Set Approach and Cross-Validation of. 

It is possible to choose among a set of models of different sizes using $C_p$, $BIC$, and adjusted $R^2$. We will consider how to do this using the validation set and cross-validation approaches.

In order for these approaches to yield accurate estimates of the test error, we must use only the *training observations* to perform all aspects of model-fitting including variable selection. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.


## Manage empty missing values

```{r}
library(ISLR)
# names(Hitters) # variablenames 
# dim(Hitters)
# sum(is.na(Hitters$Salary)) # total number of players without a recorded salary
Hitters=na.omit(Hitters)
# dim(Hitters)
# sum(is.na(Hitters))
```

## Training and Testing data split

```{r}
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters), rep=TRUE)
test <- (!train)
```

## Run Best Subset Selection Model

Apply `regsubsets()` to the training set in order to perform best subset selection.

```{r}
library(leaps)
regfit.best <- regsubsets(Salary~., data=Hitters[train,], nvmax=19)
```

For validation set error for the best model of each model size. We first make a model matrix from the test data.

```{r}
test.mat <- model.matrix(Salary~., data=Hitters[test,])
```

## Validation Set Approach

### Calculating MSE 

Now we run a loop, and for each size i, we matrix() extract the coefficients from regfit.best for the best model of that size them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.

```{r, message=FALSE, warning=FALSE}
# Trying to calculate MSE
val.errors=rep(NA,19)
for(i in 1:19){
   coefi <- coef(regfit.best,id=i)
   pred=test.mat[,names(coefi)]%*%coefi
   val.errors[i] <- mean((Hitters$Salary[test]-pred)^2)
   val.errors[i] <- mean((Hitters$Salary[test]-as.vector(pred))^2) # Either way
}
```

We find that the best model is the one that contains ten variables. Apparently with the least errors.

```{r}
val.errors
which.min(val.errors)
coef(regfit.best, 10)
```

### Best 10 variables based on full model

Since there is no `predict()` method for `regsubsets()`, and we can capture our steps above and write our own predict method.

```{r}
predict.regsubsets <- function(object,newdata,id,...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object,id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}
```

Our function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to `regsubsets()`.

Finally, we perform best subset selection on the full data set, and select the best ten-variable model, rather than simply using the variables that were obtained from the training set, because the best ten-variable model on the full data set may differ from the corresponding model on the training set.

```{r}
regfit.best <- regsubsets(Salary~., data=Hitters, nvmax=19) # full model
coef(regfit.best, 10) # select best 10
```

In fact, we see that the best ten-variable model on the full data set has a different set of variables than the best ten-variable model on the training set.

## Cross Validation Approach

We now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection *within each of the k training sets*.

First, we create a vector that allocates each observation to one of $k=10$ folds, and we create a matrix in which we will store the results.

```{r}
k=10
set.seed(1)
folds <- sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors <- matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
```


Now we write a for loop that performs cross-validation. In the $j^{th}$ fold, the elements of folds that equal $j$ are in the test set, and the remainder are in the training set.

```{r}
for(j in 1:k){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred=predict(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean( (Hitters$Salary[folds==j]-pred)^2)
    }
}
```

This has given us a $10 \times 19$ matrix, of which the $(i,j)^{th}$ element corresponds to the test MSE for the $i^{th}$ cross-validation fold for the best $j$-variable model.

We use the `apply()` function to average over the columns of this `apply()` matrix in order to obtain a vector for which the $j^{th}$ element is the cross validation error for the $j$-variable model.

```{r}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
```

We see that cross-validation selects an 11-variable model. We now perform best subset selection on the full data set in order to obtain the 11-variable model.

```{r}
reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best,11)
```