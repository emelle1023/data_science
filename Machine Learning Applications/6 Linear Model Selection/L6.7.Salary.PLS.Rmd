---
title: "Salary Prediction by Partial Least Squares"
output: html_document
---

## Synopsis

We will predict baseball players' salary using Partial Least Squares.

## Analysis

### Remove Missing data

```{r}
library(ISLR)
Hitters=na.omit(Hitters)
```


### Manage dataset in matrix for Lasso

```{r}
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
```

### Generate training and testing data for Lasso

```{r}
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
```


We implement partial least squares (PLS) using the `plsr()`.

```{r}
library(pls)
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
```

The lowest cross-validation error occurs when only M = 2 partial least squares directions are used. We now evaluate the corresponding test set MSE.

```{r}
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
```

The test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, and PCR. So, we may not prefer to use this.

Finally, we perform PLS using the full data set, using M = 2, the number of components identified by cross-validation.

```{r}
pls.fit=plsr(Salary~., data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
```

Notice that the percentage of variance in Salary that the two-component PLS fit explains, 46.40%, is almost as much as that explained using the final seven-component model PCR fit, 46.69 %. This is because PCR only attempts to maximize the amount of variance explained in the predictors, while PLS searches for directions that explain variance in both the predictors and the response.