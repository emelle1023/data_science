---
title: "Predict Wage using Polynomial Regression and Step Function and make comparsion"
output: html_document
---

## Synopsis

To predict and model wages using Polynomial Regression and Step Function and make comparison

## Analysis

### Loading the data

```{r}
library(ISLR)
attach(Wage)
```

### Fitting a model with Polynomial Regression

#### Method 1

```{r}
fit <- lm(wage~poly(age,4), data=Wage)
coef(summary(fit))
```

It seems to fit a linear model well with 4th degree polynomial.

The basis of orthogonal polynomials, which means each column is a linear combination of the variables $age$, $age^2$, $age^3$ and $age^4$.

We can also obtain $age$, $age^2$, $age^3$ and $age^4$ directly using `raw=TRUE`. But this doesn't affect the model in a meaningful way but clearly affects the coefficient estimates.

#### Method 2

```{r}
fit2 <- lm(wage~poly(age,4,raw=T), data=Wage)
coef(summary(fit2))
```

#### Method 3

```{r}
fit2a <- lm(wage~age+I(age^2)+I(age^3)+I(age^4), data=Wage)
coef(fit2a)
```

Altenatively, we can also do the same thing with the following fit, which simply creates the polynomial basis function on the fly.

#### Method 4

```{r}
fit2b <- lm(wage~cbind(age,age^2,age^3,age^4), data=Wage)
# coef(summary(fit2b))
```

Just another more compact way using `cbind` to build a matrix of vectors. Anything inside of `cbind` is a wrapper.


### Prediction using polynomial regression

We will be using the fits for prediction

```{r}
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata=list(age=age.grid), se=TRUE)
se.bands <- cbind(preds$fit + 2*preds$se.fit, preds$fit - 2*preds$se.fit)
```

### Polynomial regression model plot

Finally, we plot the data and add the fit from the degree-4 polynomial

```{r}
# par(mfrow=c(1,1), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Degree-4 Polynomial", outer=T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)
```

### Comparison of the two fits

The fitted values obtained by `fit` and `fit2` are identical, that is, as we mention, the orthognoal set of basis function produced in `poly` will not affect the model obtained in a meaningful way. 

```{r}
preds2 <- predict(fit2, newdata=list(age=age.grid), se=TRUE)
max(abs(preds$fit-preds2$fit))
```

The error is so small at `1.8332e-12`, so both `fit` and `fit2` should be identical.


### Hypothesis Test - Analysis of variance (ANOVA)

In performing a polynomial regression we must decide on the degree of the polynomial to use. One way to do this is by using hypothesis tests. We now fit models ranging from linear to a degree-5 polynomial using ANOVA.

We are using hyothesis tests to determine the simplest model of only variables `wage` and `age`. To determine which degree of models are most accurate and in order to use the ANOVA, the models must be nested.

```{r}
fit.1 <- lm(wage ~ age ,data=Wage)
fit.2 <- lm(wage ~ poly(age, 2), data=Wage)
fit.3 <- lm(wage ~ poly(age, 3), data=Wage)
fit.4 <- lm(wage ~ poly(age, 4), data=Wage)
fit.5 <- lm(wage ~ poly(age, 5), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

The p-value comparing the linear Model 1 to the quadratic Model 2 is essentially zero $(<10^{-15})$. indicating that a linear fit is not sufficient.

Similarly the p-value comparing the quadratic Model 2 to the cubic Model 3 is very low $(0.0017)$, so the quadratic fit is also insufficient.

The p-value comparing the cubic and degree-4 polynomials, Model 3 and Model 4 is approximately $5%$ while the degree-5 polynomial Model 5 seems unnecessary because its p-value is $0.37$.

Hence, either a cubic or a quartic polynomial appear to provide a reasonable fit to the data.

#### Hypothesis Test - Using poly function

Instead of using the `anova()` function, we could have obtained these p-values more succinctly by exploiting the fact that `poly()` creates orthogonal polynomials.

```
coef(summary(fit.5))
```
Notice that the p-values are the same, and in fact the square of the t-statistics are equal to the F-statistics from the `anova()` function. For example

```{r}
(-11.983)^2
```

143.5923 is the same.

However, the ANOVA method works whether or not we used orthogonal polynomials; it also works when we have other terms in the model as well.

For example, we can use ANOVA to compare these three models

```{r}
fit.1 <- lm(wage~education+age, data=Wage)
fit.2 <- lm(wage~education+poly(age,2), data=Wage)
fit.3 <- lm(wage~education+poly(age,3), data=Wage)
anova(fit.1, fit.2 ,fit.3)
```

As an alternative to using hypothesis tests and ANOVA, we could choose the polynomial degree using cross-validation.

### Polynomial logistic Regression

We are considering the task of predicting whether an individual earns more than `$250,000`. This is a binomial problem.

```{r}
fit <- glm(I(wage>250) ~ poly(age,4), data=Wage, family=binomial)
```
We use the Wrapper `I()` to create this binary response variable on the fly. The expression `wage>250` evaluates to a logical variable containing `TURE` and `FALSE`

### Predict with the polynomial logistics Regression

```{r}
preds <- predict(fit, newdata=list(age=age.grid), se=T)
```

However, calculating the confidence intervals is slightly more involved than in the linear regression case.

```{r}
pfit=exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))
```

Note that we could have directly computed the probabilities by selecting the `type="response"` option in the `predict()` function.

```{r}
preds <- predict(fit, newdata=list(age=age.grid), type="response", se=T)
```

However, the corresponding confidence intervals would not have been sensible because we would end up with negative probabilities!

### Plot of polynomial regression logistics

```{r}
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,.2))
points(jitter(age), I((wage>250)/5),cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
```

We have drawn the `age` values corresponding to the observations with `wage` values above 250 as gray marks on the top of the plot, and those with `wage` values below 250 are shown as gray marks on the bottom of the plot.

### Step Function

```{r}
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
```

Here `cut()` automatically picked the cutpoints at 33.5, 49, and 64.5 years of age. We could also have specified our own cutpoints directly using the breaks option.

The function `cut()` returns an ordered categorical variable; the `lm()` function then creates a set of dummy variables for use in the regression.

The `age<33.5` category is left out, so the intercept coefficient of `$94,160` can be interpreted as the average salary for those under 33.5 years of age. The other coefficients can be interpreted as the average additional salary for those in the other age groups.

We can produce predictions and plots just as we did in the case of the polynomial fit.