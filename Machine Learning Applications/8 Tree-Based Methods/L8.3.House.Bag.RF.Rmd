---
title: "Predict Housing Prices using Bagging and Random Forests"
output: html_document
---

## Synopsis

To check and predict housing prices using random forest, we will make comparsion to the housing prediction model from Regression Tree.

## Analysis

### Run Bagging

The randomForest() function will perform for both forests and bagging.

Bagging is just a special case of a random forest with $m=p$

```{r}
library(MASS) # For the Boston dataset
library(randomForest)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2) # 50% of random data
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, importance=TRUE)
bag.boston
```

`mtry=13` indicates that all 13 predictors should be considered for each split of the tree.

### Predict Bagging model

```{r}
boston.test <- Boston[-train,"medv"] # The response 
yhat.bag <- predict(bag.boston, newdata=Boston[-train,]) # Predict using any of the 
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2) # MSE Error
```

We see in the plot how far away the testing respones compare with the straight line.

The test set MSE associated with the bagged regression tree is 13.16, almost half that obtained using an optimally-pruned single tree

### Run and predict Random Forest with a specific number of trees

We could change the number of trees grown using the `ntree` argument.

```{r}
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, ntree=25) # we choose 25 tree
yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2) # Check MSE Error
```

We get MSE of 13.31 in the random forest model.

### Run and predict Random Forest with smaller value of `mtry`

By default, random Forest uses $\frac{1}{p}$ variables than random forest of regression trees and use $\sqrt{p}$ and we use `mtry=6` this time.

We are leaving number of trees of default.

```{r}
set.seed(1)
rf.boston <- randomForest(medv~., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf <- predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```
The test set MSE is 11.31 this indicates that random forests yielded an improvement over bagging in this case.

### To view the importane of each variable

We wil use `importance()` to see the importance of variables.

```{r}
importance(rf.boston)
```

2nd column is based upon the mean decrease of accuracy in predictions on the out of bag samples
when a given variable is excluded from the model. 

3rd column is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees

### Plot of importance measures

```{r}
varImpPlot(rf.boston)
```

The results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables.